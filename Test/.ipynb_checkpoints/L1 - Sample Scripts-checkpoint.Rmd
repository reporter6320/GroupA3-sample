---
title: "L1 series"
author: "Adam Darwich"
date: "2025-06-26"
output: html_document
---

# DISTRIBUTIONS
plottiRng PDFs and CFDs for a normal distribution:
```{r}
# a normal distribution has two parameters
# mu, the mean, and sigma^2, the variance:
mu <- 0
sigma <- 1

# we device a vector of n x:
xvec <- seq(-5,5,0.01)

# this is the pdf of a normal distribution:
fx <- ( 1 / sqrt(2*pi*sigma^2)) * exp(-(xvec-mu)^2/(2*sigma^2))

# we can then create a very simple line plot:
plot(xvec, fx, type = "l", lty = 1)

# equally, we can use the built-in functions
# the pdf function:
xpdf <- dnorm(xvec, mu, sigma)

# the cdf function:
xcdf <- pnorm(xvec, mu, sigma)

# create a simple plot of these - are they consistent?
# the built in pdf function and the "home-made" version:
plot
(xvec, fx, type = "l", lty = 1, col="red", ylab = "p(x)", xlab = "x")
lines(xvec, xpdf, type = "l", lty = 1, col="blue")
legend("topleft", legend=c("fx","pdf"), col=c("red","blue"), lty = 1, cex = 1)

# plotting the cdf:
plot(xvec, xcdf, type = "l", lty = 1, col="red", ylab = "p(x)", xlab = "x")

```

## EXAMPLE DISTRIBUTIONS
```{r}

## Normal distribution
library(NHANES)
library(dplyr)

# Letâ€™s filter the data for adult males (more on why later)
nhanes_subset <- NHANES %>%
  filter(Age > 18 & Gender == "male")

# It is reasonable to check N observations to make sure this is still substantial
n_height <- sum(complete.cases(nhanes_subset$Height))

# Then, we can plot the height data as a histogram
hist(nhanes_subset$Height, xlab = "Height [cm]", 
ylab = "Frequency", main = paste("NHANES: Height (N =", n_height,")", sep = " "))

## Lognormal distribution
n_bmi <- sum(complete.cases(nhanes_subset$BMI))

hist(nhanes_subset$BMI, breaks = seq(min(nhanes_subset$BMI, na.rm = TRUE), max(nhanes_subset$BMI, na.rm = TRUE), length.out = 25), xlab = "BMI [kg/m^2]", ylab = "Frequency", main = paste("NHANES: BMI (N =", n_bmi,")"))

## Exponential distribution
library(survival)
hist(lung.time)

library(MASS)
melanoma_subset <- Melanoma %>%
  filter(ulcer > 0)

n_lung <- sum(complete.cases(lung2$time))

hist(lung2$time, breaks = seq(min(lung2$time, na.rm = TRUE), max(lung2$time, na.rm = TRUE), length.out = 7), xlab = "Survival time [days]", ylab = "Frequency", main = paste("Lung cancer data (N = ",n_lung,")"))

## Uniform distribution

## Binomial distribution
# we can use the binomial pmf to generate data for the success of a hypothetical surgery
# e.g., p: 0.981, n: 421 (based on Fazlinovic et al. 2021)
surgery_stats <- rbinom(421,1,0.981)
n_count <- length(surgery_stats)

# we can plot the data using for example a bar plot grouped by successes and failures:
count_outcome <- table(surgery_stats)
barplot(count_outcome, names=c("deceased","survived"), main = paste("30-day outcome (N=",n_count,"patients)"),
        xlab = "Outcome", ylab = "Frequency")

# we can then get the proportions of the two outcomes
probs <- prop.table(count_outcome)

# the probability of survival is then:
p_success <- as.numeric(probs[2])

# we can then plot the probability mass function for a binomial distribution
# in this case, the probability of number of successes given 100 trials
pmf_binom <- dbinom(x = c(0:100), size = 100, prob = p_success)
plot(0:100, pmf_binom, type = "l", ylab ="Probabilities")

## Poisson distribution
# we can simulate emergency department arrival data based on Reboredo et al. 2023
# The study indicates mean/gamma arrivals of 400.96 patients during 2015-2020
n_samples <- 1000
dat_arrival <- rpois(n_samples, 400.96)

hist(dat_arrival, xlab = "Number entries", ylab = "Frequency", main = paste("ED arrival data (N = ",n_samples,")"))


```

## LAW OF LARGE NUMBERS & CENTRAL LIMIT THEOREM
```{r}
## Law of Large Numbers (LLN)
#  Let's use adult male height from NHANES to illustrate the LLN:
library(NHANES)
library(dplyr)
set.seed(12345)

# Filter the data for adult males (more on why later)
nhanes_subset <- NHANES %>%
  filter(Age > 18 & Gender == "male")

n_height <- sum(complete.cases(nhanes_subset$Height))

# We calculate the parameters for the normal distribution
height_mu <- mean(nhanes_subset$Height, na.rm = TRUE)
height_sd <- sd(nhanes_subset$Height, na.rm = TRUE)

# Let's sample up to a sample size of 10000 samples
n_iter <- 10000
height_vec <- rep(NA, n_iter)

for(i in 1:n_iter){
  height_sample <- rnorm(i,height_mu,height_sd)
  height_vec[i] <- mean(height_sample)
}

# We can then plot the average height vs sample size
plot(x = 1:n_iter, height_vec, type = "l", xlab = "sample size [n]", ylab = "average height [cm]")

## The Central Limit Theorem (CLT)
# Let's test it out
# We have a uniformly distributed variable x = U(a,b)
ua <- 0
ub <- 1

# Let's start by sampling from the uniform distribution
n_vec <- runif(10000,ua,ub)
hist(n_vec, xlab ="Value", ylab="Frequency", main="Sampling (n=10,000) from a uniform distribution")

# We continuously sample from U(a,b) and calculate the mean
n_samples <- 50
n_iter <- 100
x_vec <- rep(NA,n_iter)

for(i in 1:n_iter){
  x_sample <- runif(n_samples,ua,ub)
  x_vec[i] <- mean(x_sample)
}

hist(x_vec, xlab = "sample means", ylab = "Frequency", main = "Sampling means (n=100*50) from a uniform distribution")

```

## DISTRIBUTIONS: ESTIMATION & SIMULATION/SAMPLING
```{r}
## Distribution Estimation
#  Let's use adult male height from NHANES:
library(NHANES)
library(dplyr)
library(MASS)
set.seed(12345)

# Filter the data for adult males
nhanes_subset <- NHANES %>%
  filter(Age > 18 & Gender == "male")
n_height <- sum(complete.cases(nhanes_subset$Height))

nhanes_height <- na.omit(nhanes_subset$Height)

# Use the package MASS to fit a normal distribution to the data
# This will make use of the closed form solution
normal_params <- fitdistr(nhanes_height, "normal")

# Distributions like the Weibull require numerical optimisation
# we can test this out as well (note that this may be sensitive to initial estimates)
weibull_params <- fitdistr(nhanes_height, "weibull", list(shape = 20, scale = 200))

# We can then produce a plot of the data and the two pdfs of the fitted distributions
# First we need to specify a few things...
# set inputs for pdfs
height_min <- min(nhanes_height)
height_max <- max(nhanes_height)
x_pdf <- seq(height_min, height_max, 1)

# Then we sample from the pdfs using the parameter estimates
y_norm <- dnorm(x_pdf, normal_params$estimate["mean"], normal_params$estimate["sd"])
y_weib <- dweibull(x_pdf, weibull_params$estimate["shape"], weibull_params$estimate["scale"])

# we start with a histogram, specifying to plot the density (freq = FALSE)
hist(nhanes_height, freq = FALSE, xlab = "Height [cm]", ylab = "Density", main = "NHANES height")
lines(x_pdf, y_norm, col="blue")
lines(x_pdf, y_weib, col="red")
legend(x = "topleft", c("normal","Weibull"), col=c("blue","red"), lty = 1, cex = 1)

## Distribution Sampling
# to sample from distributions we can use the rnorm and rweibull functions, super easy:
n_samples <- 1000
norm_sample <- rnorm(n_samples, normal_params$estimate["mean"], normal_params$estimate["sd"])
weib_sample <- rweibull(n_samples, weibull_params$estimate["shape"], weibull_params$estimate["scale"])

# we can then plot these as two overlaying histograms
color1 <- rgb(20, 23, 219, max = 255, alpha = 100) # blue
color2 <- rgb(219, 20, 20, max = 255, alpha = 100) # red

h1 <- hist(norm_sample, plot = FALSE)
h2 <- hist(weib_sample, plot = FALSE)

plot(h1, col = color1, xlab = "Height [cm]", ylab = "Frequency", main = "Sampling")
plot(h2, col = color2, add = TRUE)
legend(x = "topleft", c("normal","Weibull"), fill=c("blue","red"))


```

## HYPOTHESIS TESTING:
```{r}
# Student's t-test Guinness data
yield_nkd = c(1903, 1935, 1910, 2496, 2108, 1961, 2060, 1444, 1612, 1316, 1511)
yield_kd = c(2009, 1915, 2011, 2463, 2180, 1925, 2122, 1482, 1542, 1443, 1535)

n_nkd <- length(yield_nkd)
x_nkd <- mean(yield_nkd)
sigma_nkd <- sd(yield_nkd)

n_kd <- length(yield_kd)
x_kd <- mean(yield_kd)
sigma_kd <- sd(yield_kd)

df <- n_kd + n_nkd - 2

var_tot <- ((n_nkd - 1)*sigma_nkd^2 + (n_kd - 1)*sigma_kd^2) / (n_nkd + n_kd - 2)

tstat <- (x_nkd - x_kd) /(sqrt(var_tot*((1/n_nkd)+(1/n_kd))))

# at the degrees of freedom (df=20) we get a t-statistics of 0.234
# compared to the t-distribution table at 95% two-sided, 0.234 is smaller than
# 2.086. In fact, 0.234 is smaller than 50%. So, the p-value is larger than
# 0.50. Therefore, we do not reject the null hypothesis.
# table: https://en.wikipedia.org/wiki/Student%27s_t-distribution

# we can also retrieve the p-value using the following function:
p_value <- 2 * pt(abs(tstat), df, lower.tail = FALSE)

# Let's verify our calculations using R's built-in function (or the other way around :-) ):
stat_res <- t.test(yield_kd,yield_nkd, alternative = "two.sided", paired = FALSE, var.equal = TRUE, conf.level = 0.95)
# comment: indeed, we get a p-value of 0.8173 and can therefore not reject the null hypothesis.

```

## VISUALISING DATA:
```{r}
# To better understand data in an exploratory fashion we can visualise it:
## normality
#  Let's take a look at the height data in NHANES again
library(NHANES)

# Filter the data for adult males
nhanes_subset <- NHANES %>%
  filter(Age > 18 & Gender == "male")
n_height <- sum(complete.cases(nhanes_subset$Height))

nhanes_height <- na.omit(nhanes_subset$Height)

# q-q plot: there are many functions for these, for example:
library(car)
qqPlot(nhanes_height, xlab = "normal quantiles", ylab ="NHANES: height [cm]")

# boxplot
boxplot(nhanes_height, xlab = "NHANES: adult males", ylab = "Height [cm]")

# histogram
normal_params <- fitdistr(nhanes_height, "normal")

height_min <- min(nhanes_height)
height_max <- max(nhanes_height)
x_pdf <- seq(height_min, height_max, 1)

y_norm <- dnorm(x_pdf, normal_params$estimate["mean"], normal_params$estimate["sd"])

hist(nhanes_height, freq = FALSE, xlab = "Height [cm]", ylab = "Density", main = "NHANES height")
lines(x_pdf, y_norm, col="blue")
legend(x = "topleft", c("normal"), col=c("blue"), lty = 1, cex = 1)


```

## NONPARAMETRIC TESTS I
```{r}
## The Sign Test with Hypothetical Naevus Data
#  Inspired by: https://onlinelibrary.wiley.com/doi/epdf/10.1111/bjd.14216
set.seed(34567)
#  Let's assume that Naevus count follows a Poisson distribution
#  Right arm gamma = 4.87
n_samples <- 15
right <- rpois(n_samples, 4.87)
diff <- rnorm(n_samples, mean = (5.04-4.87), sd = 1)
left <- round(right+diff)

data_sign <- sign(left - right)
dat_success <- sum(data_sign > 0)

# the sign test:
binom.test(dat_success, n_samples, alternative ="two.sided", conf.level = 0.95)

# the Wilcoxon signed-rank test
wilcox.test(right, left, paired = TRUE, exact = TRUE)

## Proportion of Large Samples test
# Let's input Arbuthnott's (the inventor of the sign test) actual dataset
# We will then do a test of proportion to analyse it

# sex birth data Arbuthnott 1710:
year = seq(1629,1710)
males = c(5218, 4858, 4422, 4994, 5158, 5035, 5106, 4917, 4703, 5359, 5366, 5518, 5470, 
          5460, 4793, 4107, 4047, 3768, 3796, 3363, 3079, 2890, 3231, 3220, 3196, 3441, 
          3655, 3668, 3396, 3157, 3209, 3724, 4748, 5216, 5411, 6041, 5114, 4678, 5616, 
          6073, 6506, 6278, 6449, 6443, 6073, 6113, 6058, 6552, 6423, 6568, 6247, 6548, 
          6822, 6909, 7577, 7575, 7484, 7575, 7737, 7487, 7604, 7909, 7662, 7602, 7676,
          6985, 7263, 7632, 8062, 8426, 7911, 7578, 8102, 8031, 7765, 6113, 8366, 7952,
          8379, 8239, 7840, 7640)
females = c(4683, 4457, 4102, 4590, 4839, 4820, 4928, 4605, 4457, 4952, 4784, 5332, 5200,
          4910, 4617, 3997, 3919, 3395, 3536, 3181, 2746, 2722, 2840, 2908, 2959, 3179,
          3349, 3382, 3289, 3013, 2781, 3247, 4107, 4803, 4881, 5681, 4858, 4319, 5322,
          5560, 5829, 5719, 6061, 6120, 5822, 5738, 5717, 5847, 6203, 6033, 6041, 6299,
          6533, 6744, 7158, 7127, 7246, 7119, 7214, 7101, 7167, 7302, 7392, 7316, 7483,
          6647, 6713, 7229, 7767, 7626, 7452, 7061, 7514, 7656, 7683, 5738, 7779, 7417,
          7687, 7623, 7380, 7288)

prop = males / (females + males)

dat_birth <- data.frame(year, males, females, prop)
n_successes <- sum(sign(dat_birth$males - dat_birth$females) > 0)
n_trials <- length(dat_birth$year)

# proportion test
prop.test(n_successes, n_trials, alternative = "two.sided", correct = TRUE)

# the sample estimate of the proportion of "successes" is 1. Let's plot the data:
plot(dat_birth$year, dat_birth$prop, xlab = "Year", ylab = "Proportion Males", ylim = c(0.5,0.55))

```

## NONPARAMETRIC TESTS II
```{r}
## Permutation test
# simulate CGAS data inspired by ref: https://bmcpsychology.biomedcentral.com/articles/10.1186/s40359-020-0393-x/tables/4
set.seed(34567)

n_samples <- 25
cgas_control <- rpois(n_samples, 52.08)
cgas_treatment <- rpois(n_samples, 60.68)
group <- rep(c("control","treat"), each = n_samples)
data_cgas <- data.frame(c(cgas_control, cgas_treatment),group)
names(data_cgas) <- c("CGAS","Group")

# calculate the test statistic of observed
tstat_obs <- abs(median(data_cgas$CGAS[data_cgas$Group == "treat"]) - 
      median(data_cgas$CGAS[data_cgas$Group == "control"]))

# carry out permutation test in R
np_sample <- length(data_cgas$Group)
n_perm <- 100000
dv_perm <- data_cgas$CGAS

# permutation data out
data_permutation <- matrix(0, nrow = np_sample, ncol = n_perm)

# generate permutation samples
for(i in 1:n_perm){
  data_permutation[,i] <- sample(dv_perm, size = np_sample, replace = FALSE)
}

# calculate test-statistics
tstat_perm <- rep(0,n_perm)

for(j in 1:n_perm){
  tstat_perm[j] <- abs(median(data_permutation[data_cgas$Group == "treat",j]) - 
      median(data_permutation[data_cgas$Group == "control",j]))
}

# p-value calculation
p_value <- mean(tstat_perm >= tstat_obs)

## Bootstrapping
#  OK, we get a p-value of 0.00114, let's estimate the confidence interval
#  of the difference in CGAS score across the groups. 

# number of bootstrap samples 
n_boot <- 10000

# We calculate our test statistic, for example:
tstatb_obs <- (median(data_cgas$CGAS[data_cgas$Group == "treat"])
               - median(data_cgas$CGAS[data_cgas$Group == "control"]))

n_control <- length(data_cgas$CGAS[data_cgas$Group == "control"])
n_treat <-  length(data_cgas$CGAS[data_cgas$Group == "treat"])

# then we can carry out the bootstrap
boot_control <- matrix(sample(data_cgas$CGAS[data_cgas$Group == "control"], 
                              size = n_boot*n_control, replace = TRUE), 
                       ncol = n_boot, nrow = n_control)
boot_treat <-  matrix(sample(data_cgas$CGAS[data_cgas$Group == "treat"], 
                              size = n_boot*n_treat, replace = TRUE), 
                       ncol = n_boot, nrow = n_treat)

median_bcontrol <- apply(boot_control, 2, median)
median_btreat <- apply(boot_treat, 2, median)
boot_mediandif <- median_btreat - median_bcontrol

# Construct the 95% confidence intervals based on percentiles
b_ci95 <- quantile(boot_mediandif, c(0.025,0.5,0.975))

```


## NONPARAMETRIC TESTS II: The Wilcoxon-Mann-Whitney Test
```{r}
# Let's make use of the same simulated CGAS data
# simulate CGAS data inspired by ref: https://bmcpsychology.biomedcentral.com/articles/10.1186/s40359-020-0393-x/tables/4
set.seed(34567)

n_samples <- 25
cgas_control <- rpois(n_samples, 52.08)
cgas_treatment <- rpois(n_samples, 60.68)
group <- rep(c("control","treat"), each = n_samples)
data_cgas <- data.frame(c(cgas_control, cgas_treatment),group)
names(data_cgas) <- c("CGAS","Group")

## Carrying out the Wilcoxon-Mann-Whitney test
wilcox.test(CGAS ~ Group, data = data_cgas)



```


## SIMULATION TO ASSESS POWER AND SIGNIFICANCE
```{r}
set.seed(34567)

## POWER
# Let's test a hypothetical uniform dataset
# We would like to test mean/median difference using:
# t-test & Wilcoxon Mann Whitney test

# log-normal distribution
x_contr <- 0.4
sd_contr <- 0.49
n_control <- 15

x_exp <- 0.95
sd_exp <- 0.51
n_experiment <- 15

pval_wmw <- replicate(10000, {
  y_contr <- rlnorm(n_control, x_contr, sd_contr)
  y_exp <- rlnorm(n_experiment, x_exp, sd_exp)
  wilcox.test(y_contr, y_exp)$p.value
})
mean(pval_wmw < 0.05)

pval_ttst <- replicate(10000, {
  y_contr <- rlnorm(n_control, x_contr, sd_contr)
  y_exp <- rlnorm(n_experiment, x_exp, sd_exp)
  t.test(y_contr,y_exp, alternative = "two.sided", paired = FALSE, var.equal = FALSE)$p.value
})
mean(pval_ttst < 0.05)

## SIGNIFICANCE
set.seed(34567)
# Let's say we have a variable that we assume to be of approximately an exponential
# distribution. We can sample from the exponential:

n_repeat <- 100
n_samples <- 6
gamma_y <- 1
# for example: y_dat <- rexp(n_samples, rate = 1)

# we calculate the t-ratio of 100 replicates
t_y <- replicate(n_repeat, {
  y_dat <- rexp(n_samples, rate = gamma_y)
  x_y <- mean(y_dat)
  sd_y <- sd(y_dat)
  t_y <-(x_y - gamma_y)/(sd_y/sqrt(n_samples))
})

# Plot the histogram and overlay the theoretical pdf
df_y <- n_samples - 1
x_tdist <- seq(-4,4,0.1)
tdist_pdf <- dt(x_tdist, df_y)
hist(t_y, freq = FALSE, xlab = "t ratio (n=6)", ylab = "Density", main = "", xlim = c(-8,8), ylim = c(0,0.5))
lines(x_tdist, tdist_pdf)



```

## STUDY DESIGN
```{r}

set.seed(34567)
library(pwr) # load the library pwr: https://cran.r-project.org/web/packages/pwr/pwr.pdf

## Clozapine-induced tachycardia
# According to Persson et al. 2024, heart rate as bpm is approximately normal
# The mean (sd) heart rate is 73 (9) bpm. Under that assumption we can
# sample from the theorical distribution.

# Normal sampling
x_contr <- 73
sd_contr <- 9
nc_samples <- 1000
bpm_contr <- rnorm(nc_samples, x_contr, sd_contr)

# Data is continuous and approximately normal, suggesting the use of a
# normal parametric t-test. 
# Tachycardia means a bpm of 100, but not the whole day.
# For simplicity, let's say we want to detect a difference 10 bpm.
x_treat <- 83
sd_treat <- 9

# calculate Cohen's D effect size
sd_pooled <- sqrt((sd_treat^2+sd_contr^2)/2)
es_d <- (x_treat - x_contr)/sd_pooled

# calculate power over sample size
p.t.two <- pwr.t.test(d = es_d, power = 0.80, type = "two.sample", alternative = "two.sided")
plot(p.t.two, xlab = "sample size per group")

```


